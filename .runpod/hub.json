{
  "title": "vLLM",
  "description": "Deploy OpenAI-Compatible Blazing-Fast LLM Endpoints powered by vLLM",
  "type": "serverless",
  "category": "language",
  "iconUrl": "https://registry.npmmirror.com/@lobehub/icons-static-png/latest/files/dark/vllm-color.png",
  "config": {
    "runsOn": "GPU",
    "containerDiskInGb": 150,
    "gpuIds": "ADA_80_PRO,AMPERE_80",
    "gpuCount": 1,
    "allowedCudaVersions": ["12.9", "12.8"],
    "presets": [
      {
        "name": "deepseek-ai/deepseek-r1-distill-llama-8b",
        "defaults": {
          "MODEL_NAME": "deepseek-ai/deepseek-r1-distill-llama-8b"
        }
      }
    ],
    "env": [
      {
        "key": "MODEL_NAME",
        "input": {
          "name": "Model",
          "type": "huggingface",
          "description": "Hugging Face model name",
          "required": true
        }
      },
      {
        "key": "TRUST_REMOTE_CODE",
        "input": {
          "name": "Trust Remote Code",
          "type": "boolean",
          "description": "Trust remote code from Hugging Face.",
          "default": false,
          "advanced": true
        }
      },
      {
        "key": "DTYPE",
        "input": {
          "name": "Data Type",
          "type": "string",
          "description": "Data type for model weights and activations.",
          "options": [
            {
              "label": "auto",
              "value": "auto"
            },
            {
              "label": "half",
              "value": "half"
            },
            {
              "label": "float16",
              "value": "float16"
            },
            {
              "label": "bfloat16",
              "value": "bfloat16"
            },
            {
              "label": "float",
              "value": "float"
            },
            {
              "label": "float32",
              "value": "float32"
            }
          ],
          "default": "auto",
          "advanced": true
        }
      },
      {
        "key": "MAX_MODEL_LEN",
        "input": {
          "name": "Max Model Length",
          "type": "number",
          "description": "Model context length.",
          "default": null,
          "advanced": true
        }
      },
      {
        "key": "ENABLE_PREFIX_CACHING",
        "input": {
          "name": "Enable Prefix Caching",
          "type": "boolean",
          "description": "Enables automatic prefix caching.",
          "default": false,
          "advanced": true
        }
      },
      {
        "key": "QUANTIZATION",
        "input": {
          "name": "Quantization",
          "type": "string",
          "description": "Method used to quantize the weights.",
          "options": [
            {
              "label": "None",
              "value": "None"
            },
            {
              "label": "AWQ",
              "value": "awq"
            },
            {
              "label": "SqueezeLLM",
              "value": "squeezellm"
            },
            {
              "label": "GPTQ",
              "value": "gptq"
            }
          ],
          "advanced": true
        }
      },
      {
        "key": "ENABLE_LORA",
        "input": {
          "name": "Enable LoRA",
          "type": "boolean",
          "description": "If True, enable handling of LoRA adapters.",
          "default": false,
          "advanced": true
        }
      },
      {
        "key": "MAX_LORAS",
        "input": {
          "name": "Max LoRAs",
          "type": "number",
          "description": "Max number of LoRAs in a single batch.",
          "default": 1,
          "advanced": true
        }
      },
      {
        "key": "MAX_LORA_RANK",
        "input": {
          "name": "Max LoRA Rank",
          "type": "number",
          "description": "Max LoRA rank.",
          "default": 16,
          "advanced": true
        }
      },
      {
        "key": "LORA_DTYPE",
        "input": {
          "name": "LoRA Data Type",
          "type": "string",
          "description": "Data type for LoRA.",
          "options": [
            {
              "label": "auto",
              "value": "auto"
            },
            {
              "label": "float16",
              "value": "float16"
            },
            {
              "label": "bfloat16",
              "value": "bfloat16"
            },
            {
              "label": "float32",
              "value": "float32"
            }
          ],
          "default": "auto",
          "advanced": true
        }
      },
      {
        "key": "MAX_CPU_LORAS",
        "input": {
          "name": "Max CPU LoRAs",
          "type": "number",
          "description": "Maximum number of LoRAs to store in CPU memory.",
          "advanced": true
        }
      },
      {
        "key": "ENABLE_CHUNKED_PREFILL",
        "input": {
          "name": "Enable Chunked Prefill",
          "type": "boolean",
          "description": "Enable chunked prefill requests.",
          "default": false,
          "advanced": true
        }
      },
      {
        "key": "CUSTOM_CHAT_TEMPLATE",
        "input": {
          "name": "Custom Chat Template",
          "type": "string",
          "description": "Custom chat jinja template",
          "advanced": true
        }
      },
      {
        "key": "GPU_MEMORY_UTILIZATION",
        "input": {
          "name": "GPU Memory Utilization",
          "type": "number",
          "description": "Sets GPU VRAM utilization",
          "default": 0.85,
          "advanced": true
        }
      },
      {
        "key": "OPENAI_RESPONSE_ROLE",
        "input": {
          "name": "OpenAI Response Role",
          "type": "string",
          "description": "Role of the LLM's Response in OpenAI Chat Completions",
          "default": "assistant",
          "advanced": true
        }
      },
      {
        "key": "ENABLE_EXPERT_PARALLEL",
        "input": {
          "name": "Enable Expert Parallel",
          "type": "boolean",
          "description": "Enable Expert Parallel for MoE models",
          "default": false,
          "advanced": true
        }
      },
      {
        "key": "MODEL_REVISION",
        "input": {
          "name": "Model Revision",
          "type": "string",
          "description": "Model revision (branch) to load",
          "advanced": true
        }
      },
      {
        "key": "BASE_PATH",
        "input": {
          "name": "Base Path",
          "type": "string",
          "description": "Storage directory for Huggingface cache and model",
          "default": "/runpod-volume",
          "advanced": true
        }
      },
      {
        "key": "TOOL_CALL_PARSER",
        "input": {
          "name": "Tool Call Parser",
          "type": "string",
          "description": "Tool call parser",
          "options": [
            {
              "label": "None",
              "value": ""
            },
            {
              "label": "Hermes",
              "value": "hermes"
            },
            {
              "label": "Mistral",
              "value": "mistral"
            },
            {
              "label": "Llama3 JSON",
              "value": "llama3_json"
            },
            {
              "label": "Pythonic",
              "value": "pythonic"
            },
            {
              "label": "InternLM",
              "value": "internlm"
            }
          ],
          "default": "",
          "advanced": true
        }
      },
      {
        "key": "REASONING_PARSER",
        "input": {
          "name": "Reasoning Parser",
          "type": "string",
          "description": "Parser for reasoning-capable models (enables reasoning mode)",
          "options": [
            { "label": "None", "value": "" },
            { "label": "DeepSeek R1", "value": "deepseek_r1" },
            { "label": "Qwen3", "value": "qwen3" },
            { "label": "Granite", "value": "granite" },
            { "label": "Hunyuan A13B", "value": "hunyuan_a13b" }
          ],
          "default": "",
          "advanced": true
        }
      }
    ]
  }
}
